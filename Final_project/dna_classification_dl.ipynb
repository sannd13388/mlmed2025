{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-30T17:53:52.48615Z",
     "iopub.status.busy": "2025-03-30T17:53:52.485847Z",
     "iopub.status.idle": "2025-03-30T17:53:52.489972Z",
     "shell.execute_reply": "2025-03-30T17:53:52.489186Z",
     "shell.execute_reply.started": "2025-03-30T17:53:52.486128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T17:53:52.491581Z",
     "iopub.status.busy": "2025-03-30T17:53:52.491249Z",
     "iopub.status.idle": "2025-03-30T17:53:52.639468Z",
     "shell.execute_reply": "2025-03-30T17:53:52.638524Z",
     "shell.execute_reply.started": "2025-03-30T17:53:52.491551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Instance_Name</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td>S10</td>\n",
       "      <td>\\t\\ttactagcaatacgcttgcgttcggtggttaagtatgtataat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+</td>\n",
       "      <td>AMPC</td>\n",
       "      <td>\\t\\ttgctatcctgacagttgtcacgctgattggtgtcgttacaat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+</td>\n",
       "      <td>AROH</td>\n",
       "      <td>\\t\\tgtactagagaactagtgcattagcttatttttttgttatcat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+</td>\n",
       "      <td>DEOP2</td>\n",
       "      <td>\\taattgtgatgtgtatcgaagtgtgttgcggagtagatgttagaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+</td>\n",
       "      <td>LEU1_TRNA</td>\n",
       "      <td>\\ttcgataattaactattgacgaaaagctgaaaaccactagaatgc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-</td>\n",
       "      <td>799</td>\n",
       "      <td>\\t\\tcctcaatggcctctaaacgggtcttgaggggttttttgctga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-</td>\n",
       "      <td>987</td>\n",
       "      <td>\\t\\tgtattctcaacaagattaaccgacagattcaatctcgtggat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-</td>\n",
       "      <td>1226</td>\n",
       "      <td>\\t\\tcgcgactacgatgagatgcctgagtgcttccgttactggatt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-</td>\n",
       "      <td>794</td>\n",
       "      <td>\\t\\tctcgtcctcaatggcctctaaacgggtcttgaggggtttttt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-</td>\n",
       "      <td>1442</td>\n",
       "      <td>\\t\\ttaacattaataaataaggaggctctaatggcactcattagcc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class Instance_Name                                           Sequence\n",
       "0       +           S10  \\t\\ttactagcaatacgcttgcgttcggtggttaagtatgtataat...\n",
       "1       +          AMPC  \\t\\ttgctatcctgacagttgtcacgctgattggtgtcgttacaat...\n",
       "2       +          AROH  \\t\\tgtactagagaactagtgcattagcttatttttttgttatcat...\n",
       "3       +         DEOP2  \\taattgtgatgtgtatcgaagtgtgttgcggagtagatgttagaa...\n",
       "4       +     LEU1_TRNA  \\ttcgataattaactattgacgaaaagctgaaaaccactagaatgc...\n",
       "..    ...           ...                                                ...\n",
       "101     -           799  \\t\\tcctcaatggcctctaaacgggtcttgaggggttttttgctga...\n",
       "102     -           987  \\t\\tgtattctcaacaagattaaccgacagattcaatctcgtggat...\n",
       "103     -          1226  \\t\\tcgcgactacgatgagatgcctgagtgcttccgttactggatt...\n",
       "104     -           794  \\t\\tctcgtcctcaatggcctctaaacgggtcttgaggggtttttt...\n",
       "105     -          1442  \\t\\ttaacattaataaataaggaggctctaatggcactcattagcc...\n",
       "\n",
       "[106 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv('/kaggle/input/promoter-gene-prediction/promoters.data',names=['Class', 'Instance_Name', 'Sequence'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T17:53:52.641697Z",
     "iopub.status.busy": "2025-03-30T17:53:52.641417Z",
     "iopub.status.idle": "2025-03-30T17:53:52.65199Z",
     "shell.execute_reply": "2025-03-30T17:53:52.650643Z",
     "shell.execute_reply.started": "2025-03-30T17:53:52.641666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      TACTAGCAATACGCTTGCGTTCGGTGGTTAAGTATGTATAATGCGC...\n",
       "1      TGCTATCCTGACAGTTGTCACGCTGATTGGTGTCGTTACAATCTAA...\n",
       "2      GTACTAGAGAACTAGTGCATTAGCTTATTTTTTTGTTATCATGCTA...\n",
       "3      AATTGTGATGTGTATCGAAGTGTGTTGCGGAGTAGATGTTAGAATA...\n",
       "4      TCGATAATTAACTATTGACGAAAAGCTGAAAACCACTAGAATGCGC...\n",
       "                             ...                        \n",
       "101    CCTCAATGGCCTCTAAACGGGTCTTGAGGGGTTTTTTGCTGAAAGG...\n",
       "102    GTATTCTCAACAAGATTAACCGACAGATTCAATCTCGTGGATGGAC...\n",
       "103    CGCGACTACGATGAGATGCCTGAGTGCTTCCGTTACTGGATTGTCA...\n",
       "104    CTCGTCCTCAATGGCCTCTAAACGGGTCTTGAGGGGTTTTTTGCTG...\n",
       "105    TAACATTAATAAATAAGGAGGCTCTAATGGCACTCATTAGCCAATC...\n",
       "Name: Sequence, Length: 106, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sequence'] = [item.split(r\"\\\\\")[0].strip() for item in data['Sequence']]\n",
    "data['Sequence'] = data['Sequence'].str.upper()\n",
    "data['Sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T17:53:52.654036Z",
     "iopub.status.busy": "2025-03-30T17:53:52.653619Z",
     "iopub.status.idle": "2025-03-30T17:53:52.722707Z",
     "shell.execute_reply": "2025-03-30T17:53:52.721844Z",
     "shell.execute_reply.started": "2025-03-30T17:53:52.653999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "101    1\n",
       "102    1\n",
       "103    1\n",
       "104    1\n",
       "105    1\n",
       "Name: Class_Label, Length: 106, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class_Label'] = pd.factorize(data['Class'])[0]\n",
    "data['Class_Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T17:53:52.724202Z",
     "iopub.status.busy": "2025-03-30T17:53:52.723833Z",
     "iopub.status.idle": "2025-03-30T17:53:57.846335Z",
     "shell.execute_reply": "2025-03-30T17:53:57.845264Z",
     "shell.execute_reply.started": "2025-03-30T17:53:52.724167Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 11 files:   0%|                                 | 0/11 [00:00<?, ?it/s]Downloading 'esm_config.py' to '/kaggle/working/.cache/huggingface/download/GRchU6ChDc-ljPnvCbORaVfYBzg=.23313afb28fe512badf134e9d1ce08e405e3656c.incomplete'\n",
      "Downloading 'modeling_esm.py' to '/kaggle/working/.cache/huggingface/download/C_0n0S8TTevl5Ia3SB4cm37Ir6A=.967189e3be48c42bb5a6af4b8243b54590cbe929.incomplete'\n",
      "Downloading 'README.md' to '/kaggle/working/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.a0ce6da1d1d4f17e33f0853b2add1a67cdf6bb32.incomplete'\n",
      "Downloading 'pytorch_model.bin' to '/kaggle/working/.cache/huggingface/download/Q1p2l2BzM1m6P5jKvr8WTq1TUio=.ad9be6f4ca8c744dde5794a0cdc443eb2d76d776d9da2377f6d9fe4f4d96fcd9.incomplete'\n",
      "\n",
      "esm_config.py: 100%|███████████████████████| 14.9k/14.9k [00:00<00:00, 98.9MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/esm_config.py\n",
      "\n",
      "README.md: 100%|███████████████████████████| 6.34k/6.34k [00:00<00:00, 40.8MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/README.md\n",
      "Downloading 'config.json' to '/kaggle/working/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.0be75ceb9f26addf6712764bc6eb2f50932d1416.incomplete'\n",
      "Downloading 'jax_model/pytree_ckpt.joblib' to '/kaggle/working/.cache/huggingface/download/jax_model/WhPbg3Mb5poFO1jnu9j-VSx2ifE=.0ac579442cba184611c171631fbc91562cd34ece139f54ab48cad5c49025140e.incomplete'\n",
      "Downloading 'tokenizer_config.json' to '/kaggle/working/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.329c0d1008755f3b4a13c5c731c919a4968da554.incomplete'\n",
      "\n",
      "pytorch_model.bin:   0%|                             | 0.00/941M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:   0%|                            | 0.00/934M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "modeling_esm.py:   0%|                              | 0.00/58.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████| 1.06k/1.06k [00:00<00:00, 10.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/config.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████████| 129/129 [00:00<00:00, 1.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading 'jax_model/hyperparams.json' to '/kaggle/working/.cache/huggingface/download/jax_model/qIzbYEVuBzV8dTHEeKQ0vdZkvWs=.00c2a11058c3668709c056ddb1752764d43fe5bf.incomplete'\n",
      "Download complete. Moving file to /kaggle/working/tokenizer_config.json\n",
      "Downloading 'special_tokens_map.json' to '/kaggle/working/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.ac47d8ff84d2793441964824395dba33d5146227.incomplete'\n",
      "Downloading '.gitattributes' to '/kaggle/working/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "modeling_esm.py: 100%|█████████████████████| 58.2k/58.2k [00:00<00:00, 5.07MB/s]\n",
      "Download complete. Moving file to /kaggle/working/modeling_esm.py\n",
      "\n",
      "\n",
      "\n",
      "special_tokens_map.json: 100%|██████████████████| 101/101 [00:00<00:00, 793kB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/special_tokens_map.json\n",
      "\n",
      "\n",
      "\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 12.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "hyperparams.json:   0%|                               | 0.00/659 [00:00<?, ?B/s]Download complete. Moving file to /kaggle/working/.gitattributes\n",
      "hyperparams.json: 100%|████████████████████████| 659/659 [00:00<00:00, 5.70MB/s]\n",
      "Download complete. Moving file to /kaggle/working/jax_model/hyperparams.json\n",
      "Downloading 'vocab.txt' to '/kaggle/working/.cache/huggingface/download/E2zehc7lrIVb8gRdx7qjK54iiZY=.07d69f8c7001350597818d8853d85ef0c203bda2.incomplete'\n",
      "Fetching 11 files:   9%|██▎                      | 1/11 [00:00<00:02,  3.67it/s]\n",
      "\n",
      "pytree_ckpt.joblib:   1%|▏                  | 10.5M/934M [00:00<00:09, 95.5MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   1%|▏                   | 10.5M/941M [00:00<00:10, 85.3MB/s]\u001b[A\n",
      "\n",
      "\n",
      "vocab.txt: 100%|███████████████████████████| 28.7k/28.7k [00:00<00:00, 76.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/vocab.txt\n",
      "\n",
      "\n",
      "pytree_ckpt.joblib:   3%|▋                   | 31.5M/934M [00:00<00:06, 141MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   4%|▉                    | 41.9M/941M [00:00<00:05, 165MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:   7%|█▎                  | 62.9M/934M [00:00<00:04, 190MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   8%|█▋                   | 73.4M/941M [00:00<00:04, 204MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  10%|██                  | 94.4M/934M [00:00<00:04, 205MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  11%|██▍                   | 105M/941M [00:00<00:03, 217MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  13%|██▊                  | 126M/934M [00:00<00:03, 215MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  14%|███▏                  | 136M/941M [00:00<00:03, 228MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▉                  | 168M/941M [00:00<00:03, 238MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  17%|███▌                 | 157M/934M [00:00<00:03, 219MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  20%|████▏                | 189M/934M [00:00<00:03, 227MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  21%|████▋                 | 199M/941M [00:00<00:03, 234MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  24%|████▉                | 220M/934M [00:01<00:03, 234MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  25%|█████▍                | 231M/941M [00:01<00:02, 239MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|██████▏               | 262M/941M [00:01<00:02, 238MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  27%|█████▋               | 252M/934M [00:01<00:02, 230MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  31%|██████▊               | 294M/941M [00:01<00:02, 241MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  30%|██████▎              | 283M/934M [00:01<00:02, 234MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  35%|███████▌              | 325M/941M [00:01<00:02, 240MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  34%|███████              | 315M/934M [00:01<00:02, 228MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  38%|████████▎             | 357M/941M [00:01<00:02, 238MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  37%|███████▊             | 346M/934M [00:01<00:02, 238MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  40%|████████▍            | 377M/934M [00:01<00:02, 241MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  41%|█████████             | 388M/941M [00:01<00:02, 236MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|█████████▊            | 419M/941M [00:01<00:02, 240MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  44%|█████████▏           | 409M/934M [00:01<00:02, 235MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  48%|██████████▌           | 451M/941M [00:01<00:02, 232MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  47%|█████████▉           | 440M/934M [00:01<00:02, 229MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  51%|███████████▎          | 482M/941M [00:02<00:01, 238MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  51%|██████████▌          | 472M/934M [00:02<00:02, 230MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  55%|████████████          | 514M/941M [00:02<00:01, 242MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  54%|███████████▎         | 503M/934M [00:02<00:01, 231MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  58%|████████████▊         | 545M/941M [00:02<00:01, 246MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  57%|████████████         | 535M/934M [00:02<00:01, 235MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  61%|█████████████▍        | 577M/941M [00:02<00:01, 247MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  61%|████████████▋        | 566M/934M [00:02<00:01, 237MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  65%|██████████████▏       | 608M/941M [00:02<00:01, 241MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  64%|█████████████▍       | 598M/934M [00:02<00:01, 233MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  68%|██████████████▉       | 640M/941M [00:02<00:01, 245MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  67%|██████████████▏      | 629M/934M [00:02<00:01, 237MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  71%|███████████████▋      | 671M/941M [00:02<00:01, 237MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  71%|██████████████▊      | 661M/934M [00:02<00:01, 231MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  75%|████████████████▍     | 703M/941M [00:03<00:01, 238MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  74%|███████████████▌     | 692M/934M [00:03<00:01, 233MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  78%|█████████████████▏    | 734M/941M [00:03<00:00, 229MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  77%|████████████████▎    | 724M/934M [00:03<00:00, 228MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  81%|█████████████████▉    | 765M/941M [00:03<00:00, 228MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  81%|████████████████▉    | 755M/934M [00:03<00:00, 231MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  85%|██████████████████▋   | 797M/941M [00:03<00:00, 226MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  84%|█████████████████▋   | 786M/934M [00:03<00:00, 226MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  88%|███████████████████▍  | 828M/941M [00:03<00:00, 226MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  88%|██████████████████▍  | 818M/934M [00:03<00:00, 226MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  91%|████████████████████  | 860M/941M [00:03<00:00, 225MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  91%|███████████████████  | 849M/934M [00:03<00:00, 226MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  95%|████████████████████▊ | 891M/941M [00:03<00:00, 222MB/s]\u001b[A\n",
      "\n",
      "pytree_ckpt.joblib:  94%|███████████████████▊ | 881M/934M [00:03<00:00, 227MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  98%|█████████████████████▌| 923M/941M [00:04<00:00, 222MB/s]\u001b[A\n",
      "\n",
      "pytorch_model.bin: 100%|██████████████████████| 941M/941M [00:04<00:00, 229MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/pytorch_model.bin\n",
      "pytree_ckpt.joblib: 100%|█████████████████████| 934M/934M [00:04<00:00, 227MB/s]\n",
      "Download complete. Moving file to /kaggle/working/jax_model/pytree_ckpt.joblib\n",
      "Fetching 11 files: 100%|████████████████████████| 11/11 [00:04<00:00,  2.55it/s]\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download InstaDeepAI/nucleotide-transformer-v2-250m-multi-species --local-dir /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T18:02:40.054097Z",
     "iopub.status.busy": "2025-03-30T18:02:40.053763Z",
     "iopub.status.idle": "2025-03-30T18:02:40.067533Z",
     "shell.execute_reply": "2025-03-30T18:02:40.066864Z",
     "shell.execute_reply.started": "2025-03-30T18:02:40.054073Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(f\"Index requested: {idx}\")\n",
    "        print(f\"Raw sequence: {self.sequences[idx]}\")\n",
    "        print(f\"Raw label: {self.labels[idx]}\")\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DNAClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes, dropout_rate=0.1):\n",
    "        super(DNAClassifier, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        \n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model.esm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "        hidden_states = outputs[0] \n",
    "    \n",
    "        pooled_output = hidden_states[:, 0, :]\n",
    "    \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "    \n",
    "        return logits\n",
    "\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10, lr=2e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()  \n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():  \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():  \n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_dna_classifier.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T18:02:47.611818Z",
     "iopub.status.busy": "2025-03-30T18:02:47.611437Z",
     "iopub.status.idle": "2025-03-30T18:02:47.870028Z",
     "shell.execute_reply": "2025-03-30T18:02:47.869089Z",
     "shell.execute_reply.started": "2025-03-30T18:02:47.611788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", trust_remote_code=True)\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T18:03:19.889703Z",
     "iopub.status.busy": "2025-03-30T18:03:19.889354Z",
     "iopub.status.idle": "2025-03-30T18:03:20.051595Z",
     "shell.execute_reply": "2025-03-30T18:03:20.050332Z",
     "shell.execute_reply.started": "2025-03-30T18:03:19.889669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b9b1ff1e93e8>:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Enables mixed precision training\n",
      "Epoch 1/10 - Training:   0%|          | 0/6 [00:00<?, ?it/s]<ipython-input-15-b9b1ff1e93e8>:101: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enables mixed precision computation\n",
      "Epoch 1/10 - Training:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index requested: 79\n",
      "Raw sequence: TTAGAGAGCATGTCAGCCTCGACAACTTGCATAAATGCTTTCTTGTAGACGTGCCCT\n",
      "Raw label: 1\n",
      "Index requested: 64\n",
      "Raw sequence: CCGTTTATTTTTTCTACCCATATCCTTGAAGCGGTGTTATAATGCCGCGCCCTCGAT\n",
      "Raw label: 0\n",
      "Index requested: 0\n",
      "Raw sequence: CGACCGAAGCGAGCCTCGTCCTCAATGGCCTCTAAACGGGTCTTGAGGGGTTTTTTG\n",
      "Raw label: 1\n",
      "Index requested: 51\n",
      "Raw sequence: TAACATTAATAAATAAGGAGGCTCTAATGGCACTCATTAGCCAATCAATCAAGAACT\n",
      "Raw label: 1\n",
      "Index requested: 72\n",
      "Raw sequence: CAGCGGCAGCACGTTTCCACGCGGTGAGAGCCTCAGGATTCATGTCGATGTCTTCCG\n",
      "Raw label: 1\n",
      "Index requested: 70\n",
      "Raw sequence: GTACTAGAGAACTAGTGCATTAGCTTATTTTTTTGTTATCATGCTAACCACCCGGCG\n",
      "Raw label: 0\n",
      "Index requested: 62\n",
      "Raw sequence: CTACGGTGGGTACAATATGCTGGATGGAGATGCGTTCACTTCTGGTCTACTGACTCG\n",
      "Raw label: 1\n",
      "Index requested: 42\n",
      "Raw sequence: AACGAGTCAATCAGACCGCTTTGACTCTGGTATTACTGTGAACATTATTCGTCTCCG\n",
      "Raw label: 1\n",
      "Index requested: 14\n",
      "Raw sequence: GATCGCACGATCTGTATACTTATTTGAGTAAATTAACCCACGATCCCAGCCATTCTT\n",
      "Raw label: 0\n",
      "Index requested: 32\n",
      "Raw sequence: AAGTGCTTAGCTTCAAGGTCACGGATACGACCGAAGCGAGCCTCGTCCTCAATGGCC\n",
      "Raw label: 1\n",
      "Index requested: 12\n",
      "Raw sequence: AGGCATGTAAACGTCTTCGTAGCGCATCAGTGCTTTCTTACTGTGAGTACGCACCAG\n",
      "Raw label: 1\n",
      "Index requested: 55\n",
      "Raw sequence: GGCCAAAAAATATCTTGTACTATTTACAAAACCTATGGTAACTCTTTAGGCATTCCT\n",
      "Raw label: 0\n",
      "Index requested: 7\n",
      "Raw sequence: AGGAGGAACTACGCAAGGTTGGAACATCGGAGAGATGCCAGCCAGCGCACCTGCACG\n",
      "Raw label: 1\n",
      "Index requested: 35\n",
      "Raw sequence: AAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCG\n",
      "Raw label: 0\n",
      "Index requested: 39\n",
      "Raw sequence: CGACTTAATATACTGCGACAGGACGTCCGTTCTGTGTAAATCGCAATGAAATGGTTT\n",
      "Raw label: 0\n",
      "Index requested: 15\n",
      "Raw sequence: AGGGGCAAGGAGGATGGAAAGAGGTTGCCGTATAAAGAAACTAGAGTCCGTTTAGGT\n",
      "Raw label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 120.12 MiB is free. Process 5200 has 14.62 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 37.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ba42cbbed252>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-b9b1ff1e93e8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, lr)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Enables mixed precision computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b9b1ff1e93e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Get outputs from the base ESM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Extract the last hidden state from the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species/ebe8e9ea00908a1e5a8f289d47d95bb09aac9f19/modeling_esm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         )\n\u001b[0;32m-> 1060\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species/ebe8e9ea00908a1e5a8f289d47d95bb09aac9f19/modeling_esm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 )\n\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    738\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species/ebe8e9ea00908a1e5a8f289d47d95bb09aac9f19/modeling_esm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         )\n\u001b[0;32m--> 610\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species/ebe8e9ea00908a1e5a8f289d47d95bb09aac9f19/modeling_esm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    524\u001b[0m     ):\n\u001b[1;32m    525\u001b[0m         \u001b[0mhidden_states_ln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mhidden_states_ln\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species/ebe8e9ea00908a1e5a8f289d47d95bb09aac9f19/modeling_esm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         if (\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 120.12 MiB is free. Process 5200 has 14.62 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 37.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "sequences = list(data['Sequence'].reset_index(drop=True))\n",
    "labels = list(data['Class_Label'].reset_index(drop=True))\n",
    "\n",
    "train_seqs, val_seqs, train_labels, val_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = DNADataset(train_seqs, train_labels, tokenizer)\n",
    "val_dataset = DNADataset(val_seqs, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "num_classes = len(set(labels)) \n",
    "model = DNAClassifier(base_model, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "train_model(model, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T18:03:10.299328Z",
     "iopub.status.busy": "2025-03-30T18:03:10.299033Z",
     "iopub.status.idle": "2025-03-30T18:03:10.303264Z",
     "shell.execute_reply": "2025-03-30T18:03:10.302473Z",
     "shell.execute_reply.started": "2025-03-30T18:03:10.299305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4650368,
     "sourceId": 7914774,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
